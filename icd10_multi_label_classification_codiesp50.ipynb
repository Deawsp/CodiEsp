{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "icd10-multi-label-classification-codiesp50.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "v97uCeMHSkjw",
        "T4F--wGSSxLd",
        "TwcKyX1cYPap",
        "6YXV2dFfcwI-",
        "ILcPscF4ddFh",
        "TeRfpfchwt1e",
        "_mSAxlItpo0r",
        "Lv3veyr-EXOr"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Deawsp/CodiEsp/blob/main/icd10_multi_label_classification_codiesp50.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v97uCeMHSkjw"
      },
      "source": [
        "#Importing python libraries and preparing the environment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Jc7fZizQ6jQ",
        "outputId": "703a1da6-ccd2-4e98-953f-30cea4a5c7ad"
      },
      "source": [
        "# Installing the transformer library\n",
        "!pip install -q transformers"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 1.8MB 10.7MB/s \n",
            "\u001b[K     |████████████████████████████████| 890kB 37.4MB/s \n",
            "\u001b[K     |████████████████████████████████| 3.2MB 37.7MB/s \n",
            "\u001b[?25h  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y1d0Z1y-R3uw"
      },
      "source": [
        "# Importing ml libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn import metrics\n",
        "import transformers\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\n",
        "from transformers import BertTokenizer, BertModel, BertConfig"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hMYoLo2rSUI3"
      },
      "source": [
        "# setting up the device for GPU usage\n",
        "from torch import cuda\n",
        "device = 'cuda' if cuda.is_available() else 'cpu'"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hhjaR8sgShXV"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T4F--wGSSxLd"
      },
      "source": [
        "# Importing and Pre-Processing the domain data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gg-lCPyHTyIF",
        "outputId": "3a880507-6189-459e-d1c7-8aab3656cddd"
      },
      "source": [
        "# # # mount colab to google drive\n",
        "from google.colab import drive \n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "djLO4cOnS3CL"
      },
      "source": [
        "import zipfile\n",
        "my_zipfolder = '/content/gdrive/MyDrive/icd10_multi_label_classification/train.csv.zip'\n",
        "with zipfile.ZipFile(my_zipfolder, 'r') as zip_ref:\n",
        "  zip_ref.extractall('working_directory')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n7Fo572lTgoU"
      },
      "source": [
        "# # # Copy multiple file in google drive folder to another folder\n",
        "import os\n",
        "import shutil\n",
        "src ='/content/gdrive/MyDrive/icd_codiesp'\n",
        "src_files = os.listdir(src)\n",
        "if not os.path.exists('/content/data'):\n",
        "    os.mkdir('/content/data')\n",
        "for file_name in src_files:\n",
        "    full_file_name = os.path.join(src, file_name)\n",
        "    if os.path.isfile(full_file_name):\n",
        "        shutil.copy(full_file_name, '/content/data')"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 198
        },
        "id": "rNo2AVZZUyLc",
        "outputId": "4e1e40c4-c1e0-42e5-cefe-120885cc0df5"
      },
      "source": [
        "df = pd.read_csv('/content/data/codiEsp50.csv')\n",
        "df['list'] = df[df.columns[2:]].values.tolist() #Create new column call list \n",
        "new_df = df[['text', 'list']].copy()\n",
        "new_df.head()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>list</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>['A 49-year-old male smoker of 12 cigarettes a...</td>\n",
              "      <td>[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>['We describe the case of a 47-year-old female...</td>\n",
              "      <td>[1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>['This is a one-month-old male of Moroccan ori...</td>\n",
              "      <td>[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>['A 39-year-old woman diagnosed with complex r...</td>\n",
              "      <td>[1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>['A 3-year-old male patient, with no relevant ...</td>\n",
              "      <td>[1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                text                                               list\n",
              "0  ['A 49-year-old male smoker of 12 cigarettes a...  [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, ...\n",
              "1  ['We describe the case of a 47-year-old female...  [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
              "2  ['This is a one-month-old male of Moroccan ori...  [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, ...\n",
              "3  ['A 39-year-old woman diagnosed with complex r...  [1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
              "4  ['A 3-year-old male patient, with no relevant ...  [1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TwcKyX1cYPap"
      },
      "source": [
        "#Preparing the dataset and dataloader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ktzsdavUVBSQ"
      },
      "source": [
        "# Sections of config\n",
        "# Defining some key variables that will be used later on in the triaining\n",
        "MAX_LEN =  512 # change to 512 instead of 200\n",
        "TRAIN_BATCH_SIZE = 8\n",
        "VALID_BATCH_SIZE = 4\n",
        "EPOCHS = 100\n",
        "LEARNING_RATE = 1e-05\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased',return_dict=False)"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yzC_xzG2Fiv3"
      },
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import torch\n",
        "from pathlib import Path\n",
        "import pickle\n",
        "\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "from torch.utils.data.distributed import DistributedSampler\n",
        "\n",
        "from transformers import (WEIGHTS_NAME, BertConfig,\n",
        "                                  BertForSequenceClassification, BertTokenizer,\n",
        "                                  XLMConfig, XLMForSequenceClassification,\n",
        "                                  XLMTokenizer, XLNetConfig,\n",
        "                                  XLNetForSequenceClassification,\n",
        "                                  XLNetTokenizer)\n",
        "\n",
        "MODEL_CLASSES = {\n",
        "    'bert': (BertConfig, BertForSequenceClassification, BertTokenizer),\n",
        "    'xlnet': (XLNetConfig, XLNetForSequenceClassification, XLNetTokenizer),\n",
        "    'xlm': (XLMConfig, XLMForSequenceClassification, XLMTokenizer)\n",
        "}\n",
        "\n",
        "class InputExample(object):\n",
        "    \"\"\"A single training/test example for simple sequence classification.\"\"\"\n",
        "\n",
        "    def __init__(self, guid, text_a, text_b=None, label=None):\n",
        "        \"\"\"Constructs a InputExample.\n",
        "        Args:\n",
        "            guid: Unique id for the example.\n",
        "            text_a: string. The untokenized text of the first sequence. For single\n",
        "            sequence tasks, only this sequence must be specified.\n",
        "            text_b: (Optional) string. The untokenized text of the second sequence.\n",
        "            Only must be specified for sequence pair tasks.\n",
        "            labels: (Optional) [string]. The label of the example. This should be\n",
        "            specified for train and dev examples, but not for test examples.\n",
        "        \"\"\"\n",
        "        self.guid = guid\n",
        "        self.text_a = text_a\n",
        "        self.text_b = text_b\n",
        "        if isinstance(label, list):\n",
        "            self.label = label\n",
        "        elif label:\n",
        "            self.label = str(label)\n",
        "        else:\n",
        "            self.label = None\n",
        "\n",
        "\n",
        "class InputFeatures(object):\n",
        "    \"\"\"A single set of features of data.\"\"\"\n",
        "\n",
        "    def __init__(self, input_ids, input_mask, segment_ids, label_id):\n",
        "        self.input_ids = input_ids\n",
        "        self.input_mask = input_mask\n",
        "        self.segment_ids = segment_ids\n",
        "        self.label_id = label_id\n",
        "\n",
        "\n",
        "def _truncate_seq_pair(tokens_a, tokens_b, max_length):\n",
        "    \"\"\"Truncates a sequence pair in place to the maximum length.\"\"\"\n",
        "\n",
        "    # This is a simple heuristic which will always truncate the longer sequence\n",
        "    # one token at a time. This makes more sense than truncating an equal percent\n",
        "    # of tokens from each, since if one sequence is very short then each token\n",
        "    # that's truncated likely contains more information than a longer sequence.\n",
        "    while True:\n",
        "        total_length = len(tokens_a) + len(tokens_b)\n",
        "        if total_length <= max_length:\n",
        "            break\n",
        "        if len(tokens_a) > len(tokens_b):\n",
        "            tokens_a.pop()\n",
        "        else:\n",
        "            tokens_b.pop()\n",
        "\n",
        "\n",
        "def convert_examples_to_features(examples, label_list, max_seq_length, tokenizer):\n",
        "    \"\"\"Loads a data file into a list of `InputBatch`s.\"\"\"\n",
        "\n",
        "    label_map = {label: i for i, label in enumerate(label_list)}\n",
        "\n",
        "    features = []\n",
        "    for (ex_index, example) in enumerate(examples):\n",
        "        try:\n",
        "            tokens_a = tokenizer.tokenize(example.text_a)\n",
        "        except:\n",
        "            print(\"Cannot tokenise item {}, Text:{}\".format(\n",
        "                ex_index, example.text_a))\n",
        "\n",
        "        tokens_b = None\n",
        "        if example.text_b:\n",
        "            tokens_b = tokenizer.tokenize(example.text_b)\n",
        "            # Modifies `tokens_a` and `tokens_b` in place so that the total\n",
        "            # length is less than the specified length.\n",
        "            # Account for [CLS], [SEP], [SEP] with \"- 3\"\n",
        "            _truncate_seq_pair(tokens_a, tokens_b, max_seq_length - 3)\n",
        "        else:\n",
        "            # Account for [CLS] and [SEP] with \"- 2\"\n",
        "            if len(tokens_a) > max_seq_length - 2:\n",
        "                tokens_a = tokens_a[:(max_seq_length - 2)]\n",
        "\n",
        "        tokens = [\"[CLS]\"] + tokens_a + [\"[SEP]\"]\n",
        "        segment_ids = [0] * len(tokens)\n",
        "\n",
        "        if tokens_b:\n",
        "            tokens += tokens_b + [\"[SEP]\"]\n",
        "            segment_ids += [1] * (len(tokens_b) + 1)\n",
        "\n",
        "        input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "\n",
        "        # The mask has 1 for real tokens and 0 for padding tokens. Only real\n",
        "        # tokens are attended to.\n",
        "        input_mask = [1] * len(input_ids)\n",
        "\n",
        "        # Zero-pad up to the sequence length.\n",
        "        padding = [0] * (max_seq_length - len(input_ids))\n",
        "        input_ids += padding\n",
        "        input_mask += padding\n",
        "        segment_ids += padding\n",
        "\n",
        "        assert len(input_ids) == max_seq_length\n",
        "        assert len(input_mask) == max_seq_length\n",
        "        assert len(segment_ids) == max_seq_length\n",
        "\n",
        "        if isinstance(example.label, list):\n",
        "            label_id = []\n",
        "            for label in example.label:\n",
        "                label_id.append(float(label))\n",
        "        else:\n",
        "            if example.label != None:\n",
        "                label_id = label_map[example.label]\n",
        "            else:\n",
        "                label_id = ''\n",
        "\n",
        "        features.append(\n",
        "            InputFeatures(input_ids=input_ids,\n",
        "                          input_mask=input_mask,\n",
        "                          segment_ids=segment_ids,\n",
        "                          label_id=label_id))\n",
        "    return features\n",
        "\n",
        "\n",
        "class DataProcessor(object):\n",
        "    \"\"\"Base class for data converters for sequence classification data sets.\"\"\"\n",
        "\n",
        "    def get_train_examples(self, filename, size=-1):\n",
        "        \"\"\"Gets a collection of `InputExample`s for the train set.\"\"\"\n",
        "        raise NotImplementedError()\n",
        "\n",
        "    def get_dev_examples(self, filename, size=-1):\n",
        "        \"\"\"Gets a collection of `InputExample`s for the dev set.\"\"\"\n",
        "        raise NotImplementedError()\n",
        "\n",
        "    def get_test_examples(self, filename, size=-1):\n",
        "        \"\"\"Gets a collection of `InputExample`s for the dev set.\"\"\"\n",
        "        raise NotImplementedError()\n",
        "\n",
        "    def get_labels(self):\n",
        "        \"\"\"Gets the list of labels for this data set.\"\"\"\n",
        "        raise NotImplementedError()\n",
        "\n",
        "\n",
        "class NERTextProcessor(DataProcessor):\n",
        "\n",
        "    def __init__(self, data_dir, label_dir):\n",
        "        self.data_dir = data_dir\n",
        "        self.label_dir = label_dir\n",
        "        self.labels = None\n",
        "\n",
        "    def get_train_examples(self, filename='train.txt'):\n",
        "        \"\"\"Gets a collection of `InputExample`s for the dev set.\"\"\"\n",
        "        return self._create_examples(self.read_col_file(os.path.join(self.data_dir, filename)), \"train\")\n",
        "\n",
        "    def get_dev_examples(self, filename='val.txt', size=-1):\n",
        "        \"\"\"Gets a collection of `InputExample`s for the dev set.\"\"\"\n",
        "        return self._create_examples(self.read_col_file(os.path.join(self.data_dir, filename)), \"val\")\n",
        "\n",
        "    def get_test_examples(self, filename='test.txt', size=-1):\n",
        "        \"\"\"Gets a collection of `InputExample`s for the test set.\"\"\"\n",
        "        return self._create_examples(self.read_col_file(os.path.join(self.data_dir, filename)), \"test\")\n",
        "\n",
        "    def get_labels(self, filename='labels.csv'):\n",
        "        \"\"\"See base class.\"\"\"\n",
        "        if self.labels == None:\n",
        "            self.labels = list(pd.read_csv(os.path.join(\n",
        "                self.label_dir, filename), header=None)[0].astype('str').values)\n",
        "        return self.labels\n",
        "\n",
        "    def _create_examples(self, lines, set_type):\n",
        "        examples = []\n",
        "        for i, (sentence, label) in enumerate(lines):\n",
        "            guid = \"%s-%s\" % (set_type, i)\n",
        "            text_a = ' '.join(sentence)\n",
        "            text_b = None\n",
        "            label = label\n",
        "            examples.append(InputExample(\n",
        "                guid=guid, text_a=text_a, text_b=text_b, label=label))\n",
        "        return examples\n",
        "\n",
        "    def read_col_file(self, filename):\n",
        "        '''\n",
        "        read file\n",
        "        return format :\n",
        "        [ ['EU', 'B-ORG'], ['rejects', 'O'], ['German', 'B-MISC'], ['call', 'O'], ['to', 'O'], ['boycott', 'O'], \n",
        "        ['British', 'B-MISC'], ['lamb', 'O'], ['.', 'O'] ]\n",
        "        '''\n",
        "        f = open(filename)\n",
        "        data = []\n",
        "        sentence = []\n",
        "        label = []\n",
        "        for line in f:\n",
        "            if len(line) == 0 or line.startswith('-DOCSTART') or line[0] == \"\\n\":\n",
        "                if len(sentence) > 0:\n",
        "                    data.append((sentence, label))\n",
        "                    sentence = []\n",
        "                    label = []\n",
        "                continue\n",
        "            splits = line.split(' ')\n",
        "            sentence.append(splits[0])\n",
        "            label.append(splits[-1][:-1])\n",
        "\n",
        "        if len(sentence) > 0:\n",
        "            data.append((sentence, label))\n",
        "            sentence = []\n",
        "            label = []\n",
        "        return data\n",
        "\n",
        "\n",
        "class TextProcessor(DataProcessor):\n",
        "\n",
        "    def __init__(self, data_dir, label_dir):\n",
        "        self.data_dir = data_dir\n",
        "        self.label_dir = label_dir\n",
        "        self.labels = None\n",
        "\n",
        "    def get_train_examples(self, filename='train.csv', text_col='text', label_col='label', size=-1):\n",
        "\n",
        "        if size == -1:\n",
        "            data_df = pd.read_csv(os.path.join(self.data_dir, filename))\n",
        "\n",
        "            return self._create_examples(data_df, \"train\", text_col=text_col, label_col=label_col)\n",
        "        else:\n",
        "            data_df = pd.read_csv(os.path.join(self.data_dir, filename))\n",
        "#             data_df['comment_text'] = data_df['comment_text'].apply(cleanHtml)\n",
        "            return self._create_examples(data_df.sample(size), \"train\", text_col=text_col, label_col=label_col)\n",
        "\n",
        "    def get_dev_examples(self, filename='val.csv', text_col='text', label_col='label', size=-1):\n",
        "\n",
        "        if size == -1:\n",
        "            data_df = pd.read_csv(os.path.join(self.data_dir, filename))\n",
        "            return self._create_examples(data_df, \"dev\", text_col=text_col, label_col=label_col)\n",
        "        else:\n",
        "            data_df = pd.read_csv(os.path.join(self.data_dir, filename))\n",
        "            return self._create_examples(data_df.sample(size), \"dev\", text_col=text_col, label_col=label_col)\n",
        "\n",
        "    def get_test_examples(self, filename='val.csv', text_col='text', label_col='label', size=-1):\n",
        "        data_df = pd.read_csv(os.path.join(self.data_dir, filename))\n",
        "#         data_df['comment_text'] = data_df['comment_text'].apply(cleanHtml)\n",
        "        if size == -1:\n",
        "            return self._create_examples(data_df, \"test\",  text_col=text_col, label_col=None)\n",
        "        else:\n",
        "            return self._create_examples(data_df.sample(size), \"test\", text_col=text_col, label_col=None)\n",
        "\n",
        "    def get_labels(self, filename='labels.csv'):\n",
        "        \"\"\"See base class.\"\"\"\n",
        "        if self.labels == None:\n",
        "            self.labels = list(pd.read_csv(os.path.join(\n",
        "                self.label_dir, filename), header=None)[0].astype('str').values)\n",
        "        return self.labels\n",
        "\n",
        "    def _create_examples(self, df, set_type, text_col, label_col):\n",
        "        \"\"\"Creates examples for the training and dev sets.\"\"\"\n",
        "        if label_col == None:\n",
        "            return list(df.apply(lambda row: InputExample(guid=row.index, text_a=row[text_col], label=None), axis=1))\n",
        "        else:\n",
        "            return list(df.apply(lambda row: InputExample(guid=row.index, text_a=row[text_col], label=str(row[label_col])), axis=1))\n",
        "\n",
        "\n",
        "class MultiLabelTextProcessor(TextProcessor):\n",
        "\n",
        "    def _create_examples(self, df, set_type, text_col, label_col):\n",
        "        def _get_labels(row, label_col):\n",
        "            if isinstance(label_col, list):\n",
        "                return list(row[label_col])\n",
        "            else:\n",
        "                # create one hot vector of labels\n",
        "                label_list = self.get_labels()\n",
        "                labels = [0] * len(label_list)\n",
        "                labels[label_list.index(row[label_col])] = 1\n",
        "                return labels\n",
        "\n",
        "        \"\"\"Creates examples for the training and dev sets.\"\"\"\n",
        "        if label_col == None:\n",
        "            return list(df.apply(lambda row: InputExample(guid=row.index, text_a=row[text_col], label=[]), axis=1))\n",
        "        else:\n",
        "            return list(df.apply(lambda row: InputExample(guid=row.index, text_a=row[text_col],\n",
        "                                                          label=_get_labels(row, label_col)), axis=1))\n",
        "\n",
        "\n",
        "class BertDataBunch(object):\n",
        "\n",
        "    def get_dl_from_texts(self, texts):\n",
        "\n",
        "        test_examples = []\n",
        "        input_data = []\n",
        "\n",
        "        for index, text in enumerate(texts):\n",
        "            test_examples.append(InputExample(index, text, label=None))\n",
        "            input_data.append({\n",
        "                'id': index,\n",
        "                'text': text\n",
        "            })\n",
        "        test_features = convert_examples_to_features(test_examples, label_list=self.labels,\n",
        "                                                     tokenizer=self.tokenizer, max_seq_length=self.maxlen)\n",
        "\n",
        "        all_input_ids = torch.tensor(\n",
        "            [f.input_ids for f in test_features], dtype=torch.long)\n",
        "        all_input_mask = torch.tensor(\n",
        "            [f.input_mask for f in test_features], dtype=torch.long)\n",
        "        all_segment_ids = torch.tensor(\n",
        "            [f.segment_ids for f in test_features], dtype=torch.long)\n",
        "\n",
        "        test_data = TensorDataset(\n",
        "            all_input_ids, all_input_mask, all_segment_ids)\n",
        "\n",
        "        test_sampler = SequentialSampler(test_data)\n",
        "        return DataLoader(test_data, sampler=test_sampler, batch_size=self.bs)\n",
        "\n",
        "    def save(self, filename=\"databunch.pkl\"):\n",
        "        tmp_path = self.data_dir/'tmp'\n",
        "        tmp_path.mkdir(exist_ok=True)\n",
        "        with open(str(tmp_path/filename), \"wb\") as f:\n",
        "            pickle.dump(self, f)\n",
        "\n",
        "    @staticmethod\n",
        "    def load(data_dir, backend='nccl', filename=\"databunch.pkl\"):\n",
        "\n",
        "        try:\n",
        "            torch.distributed.init_process_group(backend=backend,\n",
        "                                                 init_method=\"tcp://localhost:23459\",\n",
        "                                                 rank=0, world_size=1)\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "        tmp_path = data_dir/'tmp'\n",
        "        with open(str(tmp_path/filename), \"rb\") as f:\n",
        "            databunch = pickle.load(f)\n",
        "\n",
        "        return databunch\n",
        "\n",
        "    def __init__(self, data_dir, label_dir, tokenizer, train_file='train.csv', val_file='val.csv', test_data=None,\n",
        "                 label_file='labels.csv', text_col='text', label_col='label', bs=32, maxlen=512,\n",
        "                 multi_gpu=True, multi_label=False, backend=\"nccl\", model_type='bert', custom_sampler=None):\n",
        "        \n",
        "        if isinstance(tokenizer, str):\n",
        "            _,_,tokenizer_class = MODEL_CLASSES[model_type]\n",
        "            # instantiate the new tokeniser object using the tokeniser name\n",
        "            tokenizer = tokenizer_class.from_pretrained(tokenizer, do_lower_case=('uncased' in tokenizer))\n",
        "\n",
        "        self.tokenizer = tokenizer  \n",
        "        self.data_dir = data_dir\n",
        "        self.maxlen = maxlen\n",
        "        self.bs = bs\n",
        "        self.train_dl = None\n",
        "        self.val_dl = None\n",
        "        self.test_dl = None\n",
        "        self.multi_label = multi_label\n",
        "        self.n_gpu = 0\n",
        "        self.custom_sampler = custom_sampler\n",
        "        if multi_gpu:\n",
        "            self.n_gpu = torch.cuda.device_count()\n",
        "\n",
        "        if multi_label:\n",
        "            processor = MultiLabelTextProcessor(data_dir, label_dir)\n",
        "        else:\n",
        "            processor = TextProcessor(data_dir, label_dir)\n",
        "\n",
        "        self.labels = processor.get_labels(label_file)\n",
        "\n",
        "        if train_file:\n",
        "            # Train DataLoader\n",
        "            train_examples = processor.get_train_examples(\n",
        "                train_file, text_col=text_col, label_col=label_col)\n",
        "            train_features = convert_examples_to_features(train_examples, label_list=self.labels,\n",
        "                                                          tokenizer=tokenizer, max_seq_length=maxlen)\n",
        "\n",
        "            all_input_ids = torch.tensor(\n",
        "                [f.input_ids for f in train_features], dtype=torch.long)\n",
        "            all_input_mask = torch.tensor(\n",
        "                [f.input_mask for f in train_features], dtype=torch.long)\n",
        "            all_segment_ids = torch.tensor(\n",
        "                [f.segment_ids for f in train_features], dtype=torch.long)\n",
        "            if multi_label:\n",
        "                all_label_ids = torch.tensor(\n",
        "                    [f.label_id for f in train_features], dtype=torch.float)\n",
        "            else:\n",
        "                all_label_ids = torch.tensor(\n",
        "                    [f.label_id for f in train_features], dtype=torch.long)\n",
        "\n",
        "            train_data = TensorDataset(\n",
        "                all_input_ids, all_input_mask, all_segment_ids, all_label_ids)\n",
        "\n",
        "            train_batch_size = bs * max(1, self.n_gpu)\n",
        "\n",
        "            if multi_gpu:\n",
        "                if self.custom_sampler is not None:\n",
        "                    train_sampler = self.custom_sampler\n",
        "                else:\n",
        "                    train_sampler = RandomSampler(train_data)\n",
        "            else:\n",
        "                try:\n",
        "#                    torch.distributed.init_process_group(backend='nccl')\n",
        "                    torch.distributed.init_process_group(backend=backend,\n",
        "                                                         init_method=\"tcp://localhost:23459\",\n",
        "                                                         rank=0, world_size=1)\n",
        "                except:\n",
        "                    pass\n",
        "                # torch.distributed.init_process_group(backend='nccl')\n",
        "                train_sampler = DistributedSampler(train_data)\n",
        "            self.train_dl = DataLoader(\n",
        "                train_data, sampler=train_sampler, batch_size=train_batch_size)\n",
        "\n",
        "        if val_file:\n",
        "            # Validation DataLoader\n",
        "            val_examples = processor.get_dev_examples(\n",
        "                val_file, text_col=text_col, label_col=label_col)\n",
        "            val_features = convert_examples_to_features(val_examples, label_list=self.labels,\n",
        "                                                        tokenizer=tokenizer, max_seq_length=maxlen)\n",
        "\n",
        "            all_input_ids = torch.tensor(\n",
        "                [f.input_ids for f in val_features], dtype=torch.long)\n",
        "            all_input_mask = torch.tensor(\n",
        "                [f.input_mask for f in val_features], dtype=torch.long)\n",
        "            all_segment_ids = torch.tensor(\n",
        "                [f.segment_ids for f in val_features], dtype=torch.long)\n",
        "            if multi_label:\n",
        "                all_label_ids = torch.tensor(\n",
        "                    [f.label_id for f in val_features], dtype=torch.float)\n",
        "            else:\n",
        "                all_label_ids = torch.tensor(\n",
        "                    [f.label_id for f in val_features], dtype=torch.long)\n",
        "\n",
        "            val_data = TensorDataset(\n",
        "                all_input_ids, all_input_mask, all_segment_ids, all_label_ids)\n",
        "\n",
        "            val_batch_size = bs * max(1, self.n_gpu)\n",
        "            if multi_gpu:\n",
        "                val_sampler = RandomSampler(val_data)\n",
        "            else:\n",
        "                try:\n",
        "#                    torch.distributed.init_process_group(backend=backend)\n",
        "                    torch.distributed.init_process_group(backend=backend,\n",
        "                                                         init_method=\"tcp://localhost:23459\",\n",
        "                                                         rank=0, world_size=1)\n",
        "                    \n",
        "                except:\n",
        "                    pass\n",
        "\n",
        "                val_sampler = DistributedSampler(val_data)\n",
        "\n",
        "            self.val_dl = DataLoader(\n",
        "                val_data, sampler=val_sampler, batch_size=val_batch_size)\n",
        "\n",
        "        if test_data:\n",
        "            test_examples = []\n",
        "            input_data = []\n",
        "\n",
        "            for index, text in enumerate(test_data):\n",
        "                test_examples.append(InputExample(index, text))\n",
        "                input_data.append({\n",
        "                    'id': index,\n",
        "                    'text': text\n",
        "                })\n",
        "\n",
        "            test_features = convert_examples_to_features(test_examples, label_list=self.labels,\n",
        "                                                         tokenizer=tokenizer, max_seq_length=maxlen)\n",
        "            all_input_ids = torch.tensor(\n",
        "                [f.input_ids for f in test_features], dtype=torch.long)\n",
        "            all_input_mask = torch.tensor(\n",
        "                [f.input_mask for f in test_features], dtype=torch.long)\n",
        "            all_segment_ids = torch.tensor(\n",
        "                [f.segment_ids for f in test_features], dtype=torch.long)\n",
        "\n",
        "            test_data = TensorDataset(\n",
        "                all_input_ids, all_input_mask, all_segment_ids)\n",
        "\n",
        "            test_sampler = SequentialSampler(test_data)\n",
        "            self.test_dl = DataLoader(\n",
        "                test_data, sampler=test_sampler, batch_size=bs)\n",
        "\n"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jGarGwKQb3YL"
      },
      "source": [
        "class CustomDataset(Dataset):\n",
        "\n",
        "    def __init__(self, dataframe, tokenizer, max_len):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.data = dataframe\n",
        "        self.text = dataframe.text\n",
        "        self.targets = self.data.list\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.text)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        text = str(self.text[index])\n",
        "        text = \" \".join(text.split())\n",
        "\n",
        "        inputs = self.tokenizer.encode_plus(\n",
        "            text,\n",
        "            None,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_len,\n",
        "            pad_to_max_length=True,\n",
        "            return_token_type_ids=True\n",
        "        )\n",
        "        ids = inputs['input_ids']\n",
        "        mask = inputs['attention_mask']\n",
        "        token_type_ids = inputs[\"token_type_ids\"]\n",
        "\n",
        "\n",
        "        return {\n",
        "            'ids': torch.tensor(ids, dtype=torch.long),\n",
        "            'mask': torch.tensor(mask, dtype=torch.long),\n",
        "            'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long),\n",
        "            'targets': torch.tensor(self.targets[index], dtype=torch.float)\n",
        "        }\n",
        "\n",
        "    "
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eFFy4m5qcdD-",
        "outputId": "56cfcf55-3c10-4be2-f9f4-45ccdfbb84b6"
      },
      "source": [
        "# Creating the dataset and dataloader for the neural network\n",
        "\n",
        "train_size = 0.8\n",
        "train_dataset=new_df.sample(frac=train_size,random_state=200)\n",
        "test_dataset=new_df.drop(train_dataset.index).reset_index(drop=True)\n",
        "train_dataset = train_dataset.reset_index(drop=True)\n",
        "\n",
        "\n",
        "print(\"FULL Dataset: {}\".format(new_df.shape))\n",
        "print(\"TRAIN Dataset: {}\".format(train_dataset.shape))\n",
        "print(\"TEST Dataset: {}\".format(test_dataset.shape))\n",
        "\n",
        "training_set = CustomDataset(train_dataset, tokenizer, MAX_LEN)\n",
        "testing_set = CustomDataset(test_dataset, tokenizer, MAX_LEN)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "FULL Dataset: (440, 2)\n",
            "TRAIN Dataset: (352, 2)\n",
            "TEST Dataset: (88, 2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wgTdGb6RcfUD"
      },
      "source": [
        "train_params = {'batch_size': TRAIN_BATCH_SIZE,\n",
        "                'shuffle': True,\n",
        "                'num_workers': 0\n",
        "                }\n",
        "\n",
        "test_params = {'batch_size': VALID_BATCH_SIZE,\n",
        "                'shuffle': True,\n",
        "                'num_workers': 0\n",
        "                }\n",
        "\n",
        "training_loader = DataLoader(training_set, **train_params)\n",
        "testing_loader = DataLoader(testing_set, **test_params)"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7O8SXx99chx6"
      },
      "source": [
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6YXV2dFfcwI-"
      },
      "source": [
        "#Creating the Neural Network for Fine Tuning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XbMR5CK-c1-m",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "530972bb-da8e-4b34-965f-dcb0d0f52cc0"
      },
      "source": [
        "# Creating the customized model, by adding a drop out and a dense layer on top of distil bert to get the final output for the model. \n",
        "\n",
        "class BERTClass(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(BERTClass, self).__init__()\n",
        "        self.l1 = transformers.BertModel.from_pretrained('bert-base-uncased', return_dict=False)\n",
        "        self.l2 = torch.nn.Dropout(0.3)\n",
        "        self.l3 = torch.nn.Linear(768, 50) #change here 6 to 7 \n",
        "    \n",
        "    def forward(self, ids, mask, token_type_ids):\n",
        "        _, output_1= self.l1(ids, attention_mask = mask, token_type_ids = token_type_ids)\n",
        "        output_2 = self.l2(output_1)\n",
        "        output = self.l3(output_2)\n",
        "        return output\n",
        "\n",
        "\n",
        "model = BERTClass()\n",
        "model.to(device)"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BERTClass(\n",
              "  (l1): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (1): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (2): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (3): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (4): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (5): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (6): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (7): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (8): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (9): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (10): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (11): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (l2): Dropout(p=0.3, inplace=False)\n",
              "  (l3): Linear(in_features=768, out_features=50, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0hT_7-tHc9iL"
      },
      "source": [
        "def loss_fn(outputs, targets):\n",
        "    return torch.nn.BCEWithLogitsLoss()(outputs, targets)"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wFqkfRr7dXiS"
      },
      "source": [
        "optimizer = torch.optim.Adam(params =  model.parameters(), lr=LEARNING_RATE)"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nfhSrLybda9W"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ILcPscF4ddFh"
      },
      "source": [
        "#Fine Tuning the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gcOOQMEHdgN0"
      },
      "source": [
        "def train(epoch):\n",
        "    model.train()\n",
        "    for _,data in enumerate(training_loader, 0):\n",
        "        ids = data['ids'].to(device, dtype = torch.long)\n",
        "        mask = data['mask'].to(device, dtype = torch.long)\n",
        "        token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n",
        "        targets = data['targets'].to(device, dtype = torch.float)\n",
        "\n",
        "        outputs = model(ids, mask, token_type_ids, )\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss = loss_fn(outputs, targets)\n",
        "        if _%5000==0:\n",
        "            print(f'Epoch: {epoch}, Loss:  {loss.item()}')\n",
        "         \n",
        "            # logger.info(f'Epoch:{epoch}, Loss: {loss.item()}'\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z74bnkqOd0UC"
      },
      "source": [
        "def validation():\n",
        "    model.eval()\n",
        "    fin_targets=[]\n",
        "    fin_outputs=[]\n",
        "    with torch.no_grad():\n",
        "        for _, data in enumerate(testing_loader, 0):\n",
        "            ids = data['ids'].to(device, dtype = torch.long)\n",
        "            mask = data['mask'].to(device, dtype = torch.long)\n",
        "            token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n",
        "            targets = data['targets'].to(device, dtype = torch.float)\n",
        "            outputs = model(ids, mask, token_type_ids)\n",
        "            fin_targets.extend(targets.cpu().detach().numpy().tolist())\n",
        "            fin_outputs.extend(torch.sigmoid(outputs).cpu().detach().numpy().tolist())\n",
        "    return fin_outputs, fin_targets"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0u6U4jYBuTv6"
      },
      "source": [
        "import logging\n",
        "import warnings                        # To ignore any warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# a function  to create and save logs in the log files\n",
        "def log(path, file):\n",
        "\n",
        "    \"\"\"[Create a log file to record the experiment's logs]\n",
        "    \n",
        "    Arguments:\n",
        "        path {string} -- path to the directory\n",
        "        file {string} -- file name\n",
        "    \n",
        "    Returns:\n",
        "        [obj] -- [logger that record logs]\n",
        "    \"\"\"\n",
        "\n",
        "    # check if the file exist\n",
        "    log_file = os.path.join(path, file)\n",
        "\n",
        "    if not os.path.isfile(log_file):\n",
        "        open(log_file, \"w+\").close()\n",
        "\n",
        "    console_logging_format = \"%(message)s\"\n",
        "    file_logging_format = \"%(message)s\"\n",
        "    \n",
        "    \n",
        "    \n",
        "\n",
        "    # in case we want loglevel and time \n",
        "    # console_logging_format = \"%(levelname)s %(message)s\"\n",
        "    # file_logging_format = \"%(levelname)s: %(asctime)s: %(message)s\"\n",
        "\n",
        "    # configure logger\n",
        "    logging.basicConfig(level=logging.INFO, format=console_logging_format)\n",
        "    logger = logging.getLogger()\n",
        "    \n",
        "    # create a file handler for output file\n",
        "    handler = logging.FileHandler(log_file)\n",
        "\n",
        "    # set the logging level for log file\n",
        "    handler.setLevel(logging.INFO)\n",
        "    \n",
        "    # create a logging format\n",
        "    formatter = logging.Formatter(file_logging_format)\n",
        "    handler.setFormatter(formatter)\n",
        "    \n",
        "\n",
        "    # add the handlers to the logger\n",
        "    logger.addHandler(handler)\n",
        "\n",
        "    return logger"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aUGGVFGL3T2m"
      },
      "source": [
        "# set a logger file\n",
        "os.mkdir(\"logs\")\n",
        "path = \"/content/logs\"\n",
        "logger = log(path, file=\"train.logs\")\n"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W5NHyjOCdqtj",
        "outputId": "26b02602-486a-4392-9132-c34a01d2847a"
      },
      "source": [
        "# logger.info(\"Start Training\")\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    train(epoch)\n",
        "    outputs, targets = validation()\n",
        "    outputs = np.array(outputs) >= 0.5\n",
        "    accuracy = metrics.accuracy_score(targets, outputs)\n",
        "    f1_score_micro = metrics.f1_score(targets, outputs, average='micro')\n",
        "    f1_score_macro = metrics.f1_score(targets, outputs, average='macro')\n",
        "\n",
        "    # logger.info(f\"{accuracy}, {f1_score_micro}, {f1_score_macro}\")\n",
        "\n",
        "    # logger.info(f\"{f1_score_micro},\")\n",
        "    # logger.info(f\"{f1_score_macro},\")\n",
        "    # logger.info(f\"Accuracy Score = {accuracy} ,\")\n",
        "    # logger.info(f\"F1 Score (Micro) = {f1_score_micro} ,\")\n",
        "    # logger.info(f\"F1 Score (Macro) = {f1_score_macro} ,\")\n",
        "\n",
        "    print(f\"Accuracy Score = {accuracy}\")\n",
        "    print(f\"F1 Score (Micro) = {f1_score_micro}\")\n",
        "    print(f\"F1 Score (Macro) = {f1_score_macro}\")\n",
        "    print()"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0, Loss:  0.74190753698349\n",
            "Accuracy Score = 0.0\n",
            "F1 Score (Micro) = 0.05714285714285714\n",
            "F1 Score (Macro) = 0.0090137398833051\n",
            "\n",
            "Epoch: 1, Loss:  0.5235307812690735\n",
            "Accuracy Score = 0.0\n",
            "F1 Score (Micro) = 0.0\n",
            "F1 Score (Macro) = 0.0\n",
            "\n",
            "Epoch: 2, Loss:  0.3997751772403717\n",
            "Accuracy Score = 0.0\n",
            "F1 Score (Micro) = 0.0\n",
            "F1 Score (Macro) = 0.0\n",
            "\n",
            "Epoch: 3, Loss:  0.3998764753341675\n",
            "Accuracy Score = 0.0\n",
            "F1 Score (Micro) = 0.0\n",
            "F1 Score (Macro) = 0.0\n",
            "\n",
            "Epoch: 4, Loss:  0.32689300179481506\n",
            "Accuracy Score = 0.0\n",
            "F1 Score (Micro) = 0.0\n",
            "F1 Score (Macro) = 0.0\n",
            "\n",
            "Epoch: 5, Loss:  0.3157314360141754\n",
            "Accuracy Score = 0.0\n",
            "F1 Score (Micro) = 0.0\n",
            "F1 Score (Macro) = 0.0\n",
            "\n",
            "Epoch: 6, Loss:  0.315310537815094\n",
            "Accuracy Score = 0.0\n",
            "F1 Score (Micro) = 0.0\n",
            "F1 Score (Macro) = 0.0\n",
            "\n",
            "Epoch: 7, Loss:  0.2531642019748688\n",
            "Accuracy Score = 0.0\n",
            "F1 Score (Micro) = 0.0\n",
            "F1 Score (Macro) = 0.0\n",
            "\n",
            "Epoch: 8, Loss:  0.26182281970977783\n",
            "Accuracy Score = 0.0\n",
            "F1 Score (Micro) = 0.0\n",
            "F1 Score (Macro) = 0.0\n",
            "\n",
            "Epoch: 9, Loss:  0.24023213982582092\n",
            "Accuracy Score = 0.0\n",
            "F1 Score (Micro) = 0.0\n",
            "F1 Score (Macro) = 0.0\n",
            "\n",
            "Epoch: 10, Loss:  0.29447779059410095\n",
            "Accuracy Score = 0.0\n",
            "F1 Score (Micro) = 0.0\n",
            "F1 Score (Macro) = 0.0\n",
            "\n",
            "Epoch: 11, Loss:  0.23003040254116058\n",
            "Accuracy Score = 0.0\n",
            "F1 Score (Micro) = 0.0\n",
            "F1 Score (Macro) = 0.0\n",
            "\n",
            "Epoch: 12, Loss:  0.24343842267990112\n",
            "Accuracy Score = 0.0\n",
            "F1 Score (Micro) = 0.0\n",
            "F1 Score (Macro) = 0.0\n",
            "\n",
            "Epoch: 13, Loss:  0.24664081633090973\n",
            "Accuracy Score = 0.0\n",
            "F1 Score (Micro) = 0.0\n",
            "F1 Score (Macro) = 0.0\n",
            "\n",
            "Epoch: 14, Loss:  0.24727065861225128\n",
            "Accuracy Score = 0.0\n",
            "F1 Score (Micro) = 0.0\n",
            "F1 Score (Macro) = 0.0\n",
            "\n",
            "Epoch: 15, Loss:  0.2785368859767914\n",
            "Accuracy Score = 0.0\n",
            "F1 Score (Micro) = 0.0\n",
            "F1 Score (Macro) = 0.0\n",
            "\n",
            "Epoch: 16, Loss:  0.24822518229484558\n",
            "Accuracy Score = 0.0\n",
            "F1 Score (Micro) = 0.0\n",
            "F1 Score (Macro) = 0.0\n",
            "\n",
            "Epoch: 17, Loss:  0.3020244538784027\n",
            "Accuracy Score = 0.0\n",
            "F1 Score (Micro) = 0.0\n",
            "F1 Score (Macro) = 0.0\n",
            "\n",
            "Epoch: 18, Loss:  0.2120123952627182\n",
            "Accuracy Score = 0.0\n",
            "F1 Score (Micro) = 0.0\n",
            "F1 Score (Macro) = 0.0\n",
            "\n",
            "Epoch: 19, Loss:  0.26332584023475647\n",
            "Accuracy Score = 0.0\n",
            "F1 Score (Micro) = 0.0\n",
            "F1 Score (Macro) = 0.0\n",
            "\n",
            "Epoch: 20, Loss:  0.22147458791732788\n",
            "Accuracy Score = 0.0\n",
            "F1 Score (Micro) = 0.0\n",
            "F1 Score (Macro) = 0.0\n",
            "\n",
            "Epoch: 21, Loss:  0.2361544370651245\n",
            "Accuracy Score = 0.0\n",
            "F1 Score (Micro) = 0.0\n",
            "F1 Score (Macro) = 0.0\n",
            "\n",
            "Epoch: 22, Loss:  0.25774648785591125\n",
            "Accuracy Score = 0.0\n",
            "F1 Score (Micro) = 0.005633802816901409\n",
            "F1 Score (Macro) = 0.0015384615384615385\n",
            "\n",
            "Epoch: 23, Loss:  0.22048752009868622\n",
            "Accuracy Score = 0.0\n",
            "F1 Score (Micro) = 0.0223463687150838\n",
            "F1 Score (Macro) = 0.005517241379310346\n",
            "\n",
            "Epoch: 24, Loss:  0.2304755598306656\n",
            "Accuracy Score = 0.0\n",
            "F1 Score (Micro) = 0.04958677685950414\n",
            "F1 Score (Macro) = 0.010588235294117647\n",
            "\n",
            "Epoch: 25, Loss:  0.2522943913936615\n",
            "Accuracy Score = 0.0\n",
            "F1 Score (Micro) = 0.054945054945054944\n",
            "F1 Score (Macro) = 0.01142857142857143\n",
            "\n",
            "Epoch: 26, Loss:  0.22013404965400696\n",
            "Accuracy Score = 0.0\n",
            "F1 Score (Micro) = 0.06027397260273973\n",
            "F1 Score (Macro) = 0.012222222222222223\n",
            "\n",
            "Epoch: 27, Loss:  0.20248249173164368\n",
            "Accuracy Score = 0.0\n",
            "F1 Score (Micro) = 0.08648648648648648\n",
            "F1 Score (Macro) = 0.019662004662004663\n",
            "\n",
            "Epoch: 28, Loss:  0.2430228590965271\n",
            "Accuracy Score = 0.0\n",
            "F1 Score (Micro) = 0.11170212765957446\n",
            "F1 Score (Macro) = 0.017872340425531916\n",
            "\n",
            "Epoch: 29, Loss:  0.18389545381069183\n",
            "Accuracy Score = 0.0\n",
            "F1 Score (Micro) = 0.0813008130081301\n",
            "F1 Score (Macro) = 0.014999999999999998\n",
            "\n",
            "Epoch: 30, Loss:  0.22580936551094055\n",
            "Accuracy Score = 0.0\n",
            "F1 Score (Micro) = 0.09164420485175202\n",
            "F1 Score (Macro) = 0.020303030303030302\n",
            "\n",
            "Epoch: 31, Loss:  0.2073335200548172\n",
            "Accuracy Score = 0.0\n",
            "F1 Score (Micro) = 0.0813008130081301\n",
            "F1 Score (Macro) = 0.014999999999999998\n",
            "\n",
            "Epoch: 32, Loss:  0.2379664033651352\n",
            "Accuracy Score = 0.0\n",
            "F1 Score (Micro) = 0.0967741935483871\n",
            "F1 Score (Macro) = 0.016744186046511626\n",
            "\n",
            "Epoch: 33, Loss:  0.24338172376155853\n",
            "Accuracy Score = 0.0\n",
            "F1 Score (Micro) = 0.0967741935483871\n",
            "F1 Score (Macro) = 0.016744186046511626\n",
            "\n",
            "Epoch: 34, Loss:  0.138687402009964\n",
            "Accuracy Score = 0.011363636363636364\n",
            "F1 Score (Micro) = 0.10133333333333333\n",
            "F1 Score (Macro) = 0.01803030303030303\n",
            "\n",
            "Epoch: 35, Loss:  0.21308459341526031\n",
            "Accuracy Score = 0.011363636363636364\n",
            "F1 Score (Micro) = 0.12073490813648292\n",
            "F1 Score (Macro) = 0.025806576402321087\n",
            "\n",
            "Epoch: 36, Loss:  0.1834404468536377\n",
            "Accuracy Score = 0.0\n",
            "F1 Score (Micro) = 0.096514745308311\n",
            "F1 Score (Macro) = 0.02327050997782705\n",
            "\n",
            "Epoch: 37, Loss:  0.17135097086429596\n",
            "Accuracy Score = 0.0\n",
            "F1 Score (Micro) = 0.178117048346056\n",
            "F1 Score (Macro) = 0.05122852049910873\n",
            "\n",
            "Epoch: 38, Loss:  0.15639308094978333\n",
            "Accuracy Score = 0.011363636363636364\n",
            "F1 Score (Micro) = 0.15025906735751296\n",
            "F1 Score (Macro) = 0.038639393939393946\n",
            "\n",
            "Epoch: 39, Loss:  0.24120253324508667\n",
            "Accuracy Score = 0.011363636363636364\n",
            "F1 Score (Micro) = 0.10160427807486631\n",
            "F1 Score (Macro) = 0.02227642276422764\n",
            "\n",
            "Epoch: 40, Loss:  0.2122202217578888\n",
            "Accuracy Score = 0.0\n",
            "F1 Score (Micro) = 0.178117048346056\n",
            "F1 Score (Macro) = 0.054989924085576264\n",
            "\n",
            "Epoch: 41, Loss:  0.18676359951496124\n",
            "Accuracy Score = 0.0\n",
            "F1 Score (Micro) = 0.18136020151133503\n",
            "F1 Score (Macro) = 0.050641973712561954\n",
            "\n",
            "Epoch: 42, Loss:  0.13878095149993896\n",
            "Accuracy Score = 0.011363636363636364\n",
            "F1 Score (Micro) = 0.188295165394402\n",
            "F1 Score (Macro) = 0.050174336774336775\n",
            "\n",
            "Epoch: 43, Loss:  0.22735713422298431\n",
            "Accuracy Score = 0.011363636363636364\n",
            "F1 Score (Micro) = 0.13648293963254593\n",
            "F1 Score (Macro) = 0.03743431760055801\n",
            "\n",
            "Epoch: 44, Loss:  0.16728347539901733\n",
            "Accuracy Score = 0.0\n",
            "F1 Score (Micro) = 0.1649484536082474\n",
            "F1 Score (Macro) = 0.04471520542949115\n",
            "\n",
            "Epoch: 45, Loss:  0.17611856758594513\n",
            "Accuracy Score = 0.0\n",
            "F1 Score (Micro) = 0.17857142857142858\n",
            "F1 Score (Macro) = 0.057564546564546556\n",
            "\n",
            "Epoch: 46, Loss:  0.15641474723815918\n",
            "Accuracy Score = 0.03409090909090909\n",
            "F1 Score (Micro) = 0.2118226600985222\n",
            "F1 Score (Macro) = 0.06322533558395628\n",
            "\n",
            "Epoch: 47, Loss:  0.1981934756040573\n",
            "Accuracy Score = 0.022727272727272728\n",
            "F1 Score (Micro) = 0.23267326732673266\n",
            "F1 Score (Macro) = 0.07337398653190483\n",
            "\n",
            "Epoch: 48, Loss:  0.14109523594379425\n",
            "Accuracy Score = 0.022727272727272728\n",
            "F1 Score (Micro) = 0.2189054726368159\n",
            "F1 Score (Macro) = 0.06276758535582064\n",
            "\n",
            "Epoch: 49, Loss:  0.20440758764743805\n",
            "Accuracy Score = 0.022727272727272728\n",
            "F1 Score (Micro) = 0.25961538461538464\n",
            "F1 Score (Macro) = 0.08140999000999001\n",
            "\n",
            "Epoch: 50, Loss:  0.11893852055072784\n",
            "Accuracy Score = 0.022727272727272728\n",
            "F1 Score (Micro) = 0.2518159806295399\n",
            "F1 Score (Macro) = 0.07842390942390942\n",
            "\n",
            "Epoch: 51, Loss:  0.1817091852426529\n",
            "Accuracy Score = 0.022727272727272728\n",
            "F1 Score (Micro) = 0.2608695652173913\n",
            "F1 Score (Macro) = 0.08498874458874459\n",
            "\n",
            "Epoch: 52, Loss:  0.18446846306324005\n",
            "Accuracy Score = 0.03409090909090909\n",
            "F1 Score (Micro) = 0.29906542056074764\n",
            "F1 Score (Macro) = 0.10078126465897365\n",
            "\n",
            "Epoch: 53, Loss:  0.14724662899971008\n",
            "Accuracy Score = 0.045454545454545456\n",
            "F1 Score (Micro) = 0.31506849315068497\n",
            "F1 Score (Macro) = 0.1030433664222994\n",
            "\n",
            "Epoch: 54, Loss:  0.13936655223369598\n",
            "Accuracy Score = 0.045454545454545456\n",
            "F1 Score (Micro) = 0.3037383177570094\n",
            "F1 Score (Macro) = 0.10127100415923945\n",
            "\n",
            "Epoch: 55, Loss:  0.16358765959739685\n",
            "Accuracy Score = 0.045454545454545456\n",
            "F1 Score (Micro) = 0.24817518248175185\n",
            "F1 Score (Macro) = 0.0780170359052712\n",
            "\n",
            "Epoch: 56, Loss:  0.13163511455059052\n",
            "Accuracy Score = 0.022727272727272728\n",
            "F1 Score (Micro) = 0.2608695652173913\n",
            "F1 Score (Macro) = 0.08327251320108463\n",
            "\n",
            "Epoch: 57, Loss:  0.1322634071111679\n",
            "Accuracy Score = 0.045454545454545456\n",
            "F1 Score (Micro) = 0.31701631701631705\n",
            "F1 Score (Macro) = 0.10764662004662005\n",
            "\n",
            "Epoch: 58, Loss:  0.12108343094587326\n",
            "Accuracy Score = 0.045454545454545456\n",
            "F1 Score (Micro) = 0.31870669745958435\n",
            "F1 Score (Macro) = 0.10774214543626309\n",
            "\n",
            "Epoch: 59, Loss:  0.11671710759401321\n",
            "Accuracy Score = 0.045454545454545456\n",
            "F1 Score (Micro) = 0.32272727272727275\n",
            "F1 Score (Macro) = 0.1075587612822907\n",
            "\n",
            "Epoch: 60, Loss:  0.15153418481349945\n",
            "Accuracy Score = 0.045454545454545456\n",
            "F1 Score (Micro) = 0.33183856502242154\n",
            "F1 Score (Macro) = 0.11643071177188824\n",
            "\n",
            "Epoch: 61, Loss:  0.14482825994491577\n",
            "Accuracy Score = 0.045454545454545456\n",
            "F1 Score (Micro) = 0.3219954648526077\n",
            "F1 Score (Macro) = 0.11594601476954416\n",
            "\n",
            "Epoch: 62, Loss:  0.15396639704704285\n",
            "Accuracy Score = 0.045454545454545456\n",
            "F1 Score (Micro) = 0.3318181818181818\n",
            "F1 Score (Macro) = 0.1201341736694678\n",
            "\n",
            "Epoch: 63, Loss:  0.15292905271053314\n",
            "Accuracy Score = 0.045454545454545456\n",
            "F1 Score (Micro) = 0.31090487238979114\n",
            "F1 Score (Macro) = 0.10510292045790766\n",
            "\n",
            "Epoch: 64, Loss:  0.18360424041748047\n",
            "Accuracy Score = 0.045454545454545456\n",
            "F1 Score (Micro) = 0.32183908045977017\n",
            "F1 Score (Macro) = 0.11728107687519453\n",
            "\n",
            "Epoch: 65, Loss:  0.11028625071048737\n",
            "Accuracy Score = 0.045454545454545456\n",
            "F1 Score (Micro) = 0.34234234234234234\n",
            "F1 Score (Macro) = 0.12861856640091937\n",
            "\n",
            "Epoch: 66, Loss:  0.11073242127895355\n",
            "Accuracy Score = 0.045454545454545456\n",
            "F1 Score (Micro) = 0.3325740318906606\n",
            "F1 Score (Macro) = 0.11611058699293991\n",
            "\n",
            "Epoch: 67, Loss:  0.14020846784114838\n",
            "Accuracy Score = 0.045454545454545456\n",
            "F1 Score (Micro) = 0.35794183445190153\n",
            "F1 Score (Macro) = 0.13399219311227423\n",
            "\n",
            "Epoch: 68, Loss:  0.15595512092113495\n",
            "Accuracy Score = 0.045454545454545456\n",
            "F1 Score (Micro) = 0.3264367816091954\n",
            "F1 Score (Macro) = 0.1135133903133903\n",
            "\n",
            "Epoch: 69, Loss:  0.12634733319282532\n",
            "Accuracy Score = 0.045454545454545456\n",
            "F1 Score (Micro) = 0.35476718403547675\n",
            "F1 Score (Macro) = 0.14522920832384487\n",
            "\n",
            "Epoch: 70, Loss:  0.15476912260055542\n",
            "Accuracy Score = 0.045454545454545456\n",
            "F1 Score (Micro) = 0.3415730337078652\n",
            "F1 Score (Macro) = 0.12302590612002379\n",
            "\n",
            "Epoch: 71, Loss:  0.11064837872982025\n",
            "Accuracy Score = 0.056818181818181816\n",
            "F1 Score (Micro) = 0.3542600896860986\n",
            "F1 Score (Macro) = 0.15282815872870625\n",
            "\n",
            "Epoch: 72, Loss:  0.11439788341522217\n",
            "Accuracy Score = 0.045454545454545456\n",
            "F1 Score (Micro) = 0.3325740318906606\n",
            "F1 Score (Macro) = 0.11989788238855176\n",
            "\n",
            "Epoch: 73, Loss:  0.16730888187885284\n",
            "Accuracy Score = 0.045454545454545456\n",
            "F1 Score (Micro) = 0.3558558558558559\n",
            "F1 Score (Macro) = 0.14267974901672378\n",
            "\n",
            "Epoch: 74, Loss:  0.11191295087337494\n",
            "Accuracy Score = 0.045454545454545456\n",
            "F1 Score (Micro) = 0.3303167420814479\n",
            "F1 Score (Macro) = 0.11749788030433915\n",
            "\n",
            "Epoch: 75, Loss:  0.14126650989055634\n",
            "Accuracy Score = 0.045454545454545456\n",
            "F1 Score (Micro) = 0.37802197802197807\n",
            "F1 Score (Macro) = 0.14932071160305457\n",
            "\n",
            "Epoch: 76, Loss:  0.1161678209900856\n",
            "Accuracy Score = 0.045454545454545456\n",
            "F1 Score (Micro) = 0.3680709534368071\n",
            "F1 Score (Macro) = 0.14867727446695558\n",
            "\n",
            "Epoch: 77, Loss:  0.10793731361627579\n",
            "Accuracy Score = 0.056818181818181816\n",
            "F1 Score (Micro) = 0.36123348017621143\n",
            "F1 Score (Macro) = 0.1391346044850689\n",
            "\n",
            "Epoch: 78, Loss:  0.12894810736179352\n",
            "Accuracy Score = 0.045454545454545456\n",
            "F1 Score (Micro) = 0.3318181818181818\n",
            "F1 Score (Macro) = 0.11775184007536948\n",
            "\n",
            "Epoch: 79, Loss:  0.10644908994436264\n",
            "Accuracy Score = 0.056818181818181816\n",
            "F1 Score (Micro) = 0.36080178173719374\n",
            "F1 Score (Macro) = 0.1529300113027667\n",
            "\n",
            "Epoch: 80, Loss:  0.10056356340646744\n",
            "Accuracy Score = 0.06818181818181818\n",
            "F1 Score (Micro) = 0.37280701754385964\n",
            "F1 Score (Macro) = 0.14816750325820535\n",
            "\n",
            "Epoch: 81, Loss:  0.12351817637681961\n",
            "Accuracy Score = 0.056818181818181816\n",
            "F1 Score (Micro) = 0.35794183445190153\n",
            "F1 Score (Macro) = 0.14477888661046556\n",
            "\n",
            "Epoch: 82, Loss:  0.08966165035963058\n",
            "Accuracy Score = 0.056818181818181816\n",
            "F1 Score (Micro) = 0.3708609271523179\n",
            "F1 Score (Macro) = 0.1389063452676356\n",
            "\n",
            "Epoch: 83, Loss:  0.07829688489437103\n",
            "Accuracy Score = 0.045454545454545456\n",
            "F1 Score (Micro) = 0.37004405286343617\n",
            "F1 Score (Macro) = 0.1615655879975029\n",
            "\n",
            "Epoch: 84, Loss:  0.09929634630680084\n",
            "Accuracy Score = 0.06818181818181818\n",
            "F1 Score (Micro) = 0.36923076923076925\n",
            "F1 Score (Macro) = 0.15631605422114048\n",
            "\n",
            "Epoch: 85, Loss:  0.10537530481815338\n",
            "Accuracy Score = 0.06818181818181818\n",
            "F1 Score (Micro) = 0.39655172413793105\n",
            "F1 Score (Macro) = 0.18480304158041755\n",
            "\n",
            "Epoch: 86, Loss:  0.11437021195888519\n",
            "Accuracy Score = 0.056818181818181816\n",
            "F1 Score (Micro) = 0.37362637362637363\n",
            "F1 Score (Macro) = 0.1609569200779727\n",
            "\n",
            "Epoch: 87, Loss:  0.09248743206262589\n",
            "Accuracy Score = 0.07954545454545454\n",
            "F1 Score (Micro) = 0.3904555314533622\n",
            "F1 Score (Macro) = 0.1735728941106223\n",
            "\n",
            "Epoch: 88, Loss:  0.0867512971162796\n",
            "Accuracy Score = 0.06818181818181818\n",
            "F1 Score (Micro) = 0.38626609442060084\n",
            "F1 Score (Macro) = 0.170739356700647\n",
            "\n",
            "Epoch: 89, Loss:  0.10762930661439896\n",
            "Accuracy Score = 0.07954545454545454\n",
            "F1 Score (Micro) = 0.3956989247311828\n",
            "F1 Score (Macro) = 0.17673719252883338\n",
            "\n",
            "Epoch: 90, Loss:  0.14251068234443665\n",
            "Accuracy Score = 0.06818181818181818\n",
            "F1 Score (Micro) = 0.3703703703703704\n",
            "F1 Score (Macro) = 0.1620071014640674\n",
            "\n",
            "Epoch: 91, Loss:  0.10700725018978119\n",
            "Accuracy Score = 0.07954545454545454\n",
            "F1 Score (Micro) = 0.3877995642701525\n",
            "F1 Score (Macro) = 0.17470345930676673\n",
            "\n",
            "Epoch: 92, Loss:  0.11213508993387222\n",
            "Accuracy Score = 0.07954545454545454\n",
            "F1 Score (Micro) = 0.40764331210191085\n",
            "F1 Score (Macro) = 0.1844621838295779\n",
            "\n",
            "Epoch: 93, Loss:  0.10324563831090927\n",
            "Accuracy Score = 0.09090909090909091\n",
            "F1 Score (Micro) = 0.4\n",
            "F1 Score (Macro) = 0.19164969279086924\n",
            "\n",
            "Epoch: 94, Loss:  0.12085725367069244\n",
            "Accuracy Score = 0.06818181818181818\n",
            "F1 Score (Micro) = 0.3656387665198238\n",
            "F1 Score (Macro) = 0.15103553815936718\n",
            "\n",
            "Epoch: 95, Loss:  0.10390918701887131\n",
            "Accuracy Score = 0.09090909090909091\n",
            "F1 Score (Micro) = 0.4143763213530655\n",
            "F1 Score (Macro) = 0.19934360118132935\n",
            "\n",
            "Epoch: 96, Loss:  0.11175888031721115\n",
            "Accuracy Score = 0.07954545454545454\n",
            "F1 Score (Micro) = 0.38095238095238093\n",
            "F1 Score (Macro) = 0.15677412972293797\n",
            "\n",
            "Epoch: 97, Loss:  0.05327695608139038\n",
            "Accuracy Score = 0.056818181818181816\n",
            "F1 Score (Micro) = 0.36761487964989065\n",
            "F1 Score (Macro) = 0.15163416558768325\n",
            "\n",
            "Epoch: 98, Loss:  0.10595565289258957\n",
            "Accuracy Score = 0.07954545454545454\n",
            "F1 Score (Micro) = 0.40256959314775165\n",
            "F1 Score (Macro) = 0.18461678141996582\n",
            "\n",
            "Epoch: 99, Loss:  0.07515119761228561\n",
            "Accuracy Score = 0.06818181818181818\n",
            "F1 Score (Micro) = 0.3774403470715835\n",
            "F1 Score (Macro) = 0.16313864008153323\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TeRfpfchwt1e"
      },
      "source": [
        "#Plot"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 198
        },
        "id": "Gt8z8qesuZ1R",
        "outputId": "737a1a36-af14-4260-83c1-64fbb15bd960"
      },
      "source": [
        "import pandas as pd\n",
        "import pylab as plt\n",
        "\n",
        "# Create dataframe\n",
        "file_name = \"/content/logs/train.logs\"\n",
        "dflog = pd.read_csv(file_name)\n",
        "# dflog.plot()\n",
        "# plt.show()\n",
        "dflog.tail()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0.3235294117647059</th>\n",
              "      <th>0.6602870813397129</th>\n",
              "      <th>0.6159376532275693</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>44</th>\n",
              "      <td>0.323529</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>0.622765</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>45</th>\n",
              "      <td>0.338235</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>0.627880</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>46</th>\n",
              "      <td>0.382353</td>\n",
              "      <td>0.681416</td>\n",
              "      <td>0.646638</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>47</th>\n",
              "      <td>0.338235</td>\n",
              "      <td>0.669725</td>\n",
              "      <td>0.634705</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>48</th>\n",
              "      <td>0.308824</td>\n",
              "      <td>0.657277</td>\n",
              "      <td>0.619133</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    0.3235294117647059   0.6602870813397129   0.6159376532275693 \n",
              "44            0.323529             0.666667              0.622765\n",
              "45            0.338235             0.666667              0.627880\n",
              "46            0.382353             0.681416              0.646638\n",
              "47            0.338235             0.669725              0.634705\n",
              "48            0.308824             0.657277              0.619133"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 83
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CdeEUjNVdnXe"
      },
      "source": [
        "def load_metrics(load_path):\n",
        "\n",
        "    if load_path==None:\n",
        "        return\n",
        "    \n",
        "    state_dict = torch.load(load_path, map_location=device)\n",
        "    print(f'Model loaded from <== {load_path}')\n",
        "    \n",
        "    return state_dict['train_loss_list'], state_dict['valid_loss_list'], state_dict['global_steps_list']\n",
        "    \n",
        "destination_folder = '/content/logs'\n",
        "train_loss_list, valid_loss_list, global_steps_list = load_metrics(destination_folder + '/train.logs')\n",
        "plt.plot(global_steps_list, train_loss_list, label='Train')\n",
        "plt.plot(global_steps_list, valid_loss_list, label='Valid')\n",
        "plt.xlabel('Global Steps')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show() "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_mSAxlItpo0r"
      },
      "source": [
        "#Validating the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pjbGRVGwvo0w"
      },
      "source": [
        "for epoch in range(EPOCHS):\n",
        "    outputs, targets = validation(epoch)\n",
        "    outputs = np.array(outputs) >= 0.5\n",
        "    accuracy = metrics.accuracy_score(targets, outputs)\n",
        "    f1_score_micro = metrics.f1_score(targets, outputs, average='micro')\n",
        "    f1_score_macro = metrics.f1_score(targets, outputs, average='macro')\n",
        "    print(f\"Accuracy Score = {accuracy}\")\n",
        "    print(f\"F1 Score (Micro) = {f1_score_micro}\")\n",
        "    print(f\"F1 Score (Macro) = {f1_score_macro}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "02AgiC3C3TM1"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lv3veyr-EXOr"
      },
      "source": [
        "#Saving the Trained Model for inference"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d7hqPmLOEhB3"
      },
      "source": [
        "import shutil\n",
        " \n",
        "if os.path.exists('/content/data/models'): \n",
        "  shutil.rmtree('/content/data/models')\n",
        "else:\n",
        "  os.mkdir('/content/data/models')\n",
        "  OUTPUT_DIR = '/content/data/models' "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oD6ZCW-IEc8R",
        "outputId": "f0074f95-f6dc-4ac2-b5c0-875071ecc5c1"
      },
      "source": [
        "# # #Saving the files for inference\n",
        "\n",
        "# output_model_file = './models/pytorch_distilbert_news.bin'\n",
        "# output_vocab_file = './models/vocab_distilbert_news.bin'\n",
        "\n",
        "if not os.path.exists('/content/data/models'):\n",
        "    os.mkdir('/content/data/models')\n",
        "\n",
        "\n",
        "output_model_file = '/content/data/models/pytorch_bert_icd.pt'\n",
        "output_vocab_file = '/content/data/models/vocab_bert_icd.pt'\n",
        "\n",
        "\n",
        "\n",
        "torch.save(model, output_model_file)\n",
        "tokenizer.save_vocabulary(output_vocab_file)\n",
        "\n",
        "print('Saved')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Saved\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CskICAtReVz3"
      },
      "source": [
        "PATH = '/content/data/models/pytorch_bert_icd.pt'\n",
        "model = torch.load(PATH)\n",
        "model.eval()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 345
        },
        "id": "gXLHadqVkIQD",
        "outputId": "ae4fddac-f35b-4518-820b-3f585f1445dc"
      },
      "source": [
        "texts = ['I really love the Netflix original movies',\n",
        "\t\t 'this movie is not worth watching']\n",
        "predictions = model.predict_batch(texts)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleAttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleAttributeError\u001b[0m                      Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-cd7958c413b2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m texts = ['I really love the Netflix original movies',\n\u001b[1;32m      2\u001b[0m \t\t 'this movie is not worth watching']\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtexts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-13-94eb164be0c9>\u001b[0m in \u001b[0;36mpredict_batch\u001b[0;34m(self, texts)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtexts\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m             \u001b[0mdl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_dl_from_texts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtexts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_dl\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m             \u001b[0mdl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_dl\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    777\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    778\u001b[0m         raise ModuleAttributeError(\"'{}' object has no attribute '{}'\".format(\n\u001b[0;32m--> 779\u001b[0;31m             type(self).__name__, name))\n\u001b[0m\u001b[1;32m    780\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    781\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Module'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleAttributeError\u001b[0m: 'BERTClass' object has no attribute 'data'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 345
        },
        "id": "gNySSHLcm4nv",
        "outputId": "133ca255-ae0b-45cd-c165-39b95d27cece"
      },
      "source": [
        "texts = ['I really love the Netflix original movies',\n",
        "\t\t 'this movie is not worth watching']\n",
        "predictions = model.predict_batch(texts)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleAttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleAttributeError\u001b[0m                      Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-38-cd7958c413b2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m texts = ['I really love the Netflix original movies',\n\u001b[1;32m      2\u001b[0m \t\t 'this movie is not worth watching']\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtexts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-25-aea621049f4e>\u001b[0m in \u001b[0;36mpredict_batch\u001b[0;34m(self, texts)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtexts\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m             \u001b[0mdl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_dl_from_texts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtexts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_dl\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m             \u001b[0mdl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_dl\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    777\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    778\u001b[0m         raise ModuleAttributeError(\"'{}' object has no attribute '{}'\".format(\n\u001b[0;32m--> 779\u001b[0;31m             type(self).__name__, name))\n\u001b[0m\u001b[1;32m    780\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    781\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Module'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleAttributeError\u001b[0m: 'BERTClass' object has no attribute 'data'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cChNwc6vo10r"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}